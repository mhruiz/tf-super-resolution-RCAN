{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cathedral-bronze",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n",
      "----------------------------------\n",
      "Num images for training: 800\n",
      "Num images for validating: 100\n",
      "----------------------------------\n",
      "Smallest image in train dataset: 1116\n",
      "Smallest image in validation dataset: 1356\n",
      "----------------------------------\n",
      "Biggest image in train dataset: 2040\n",
      "Biggest image in validation dataset: 2040\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "# GPU memory limit\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)\n",
    "\n",
    "# Import own modules\n",
    "import lib.training_loops as training_loops\n",
    "import lib.custom_callbacks as callbacks\n",
    "import lib.PrepareDataset as dt\n",
    "import lib.constants as ctes\n",
    "import lib.RCAN as RCAN\n",
    "\n",
    "######################################################################################\n",
    "# DEFINE ARGUMENTS\n",
    "\n",
    "# Load datasets from CSV files\n",
    "TRAINING_DATASETS = [pd.read_csv('data/0_csvs/DIV2K_train_HR.csv')]\n",
    "VALIDATION_DATASETS = [pd.read_csv('data/0_csvs/DIV2K_valid_HR.csv')]\n",
    "\n",
    "# Define dataset parameters\n",
    "TRAINING_COLOR_MODE = ctes.COLOR_MODE.RGB\n",
    "TRAINING_SCALE = 3\n",
    "TRAINING_BATCH_SIZE = 16\n",
    "TRAINING_PATCH_SIZE = 48\n",
    "TRAINING_DATA_AUG = True\n",
    "TRAINING_SHUFFLE = True\n",
    "TRAINING_REPEAT = 200 # ----------------- Según la implementación en TF 1.13, se valida cada 10.000 training steps. Al ser 800 imágenes de entrenamiento y un batch de 16, hay 50 steps por época\n",
    "                      #                   Hay que realizar 10.000 iteraciones antes de validar. Estas son equivalentes a 200 épocas de entrenamiento sin validación, osea, repetir 200 veces el dataset\n",
    "TRAINING_NORMALIZE = False\n",
    "\n",
    "VALIDATION_COLOR_MODE = TRAINING_COLOR_MODE\n",
    "VALIDATION_SCALE = TRAINING_SCALE\n",
    "VALIDATION_BATCH_SIZE = 1\n",
    "VALIDATION_PATCH_SIZE = None\n",
    "VALIDATION_DATA_AUG = False\n",
    "VALIDATION_SHUFFLE = False\n",
    "VALIDATION_REPEAT = None\n",
    "VALIDATION_NORMALIZE = False\n",
    "\n",
    "FILENAME = 'nb05_train_rcan-scale3_loss-computed-on-lr'\n",
    "\n",
    "# Get model info from file name\n",
    "info_from_script_name = FILENAME.split('_') \n",
    "\n",
    "model_structure_name = info_from_script_name[2].upper() \n",
    "\n",
    "model_name = info_from_script_name[3:]            \n",
    "model_name = ('_'.join(['model'] + model_name)).split('.')[0]\n",
    "model_name_for_metrics = model_structure_name + '_' + model_name         \n",
    "\n",
    "BASE_MODEL_PATH = 'TRAINED_MODELS_NEW/'\n",
    "BASE_METRIC_PATH = 'TRAINING_METRIC_EVOLUTIONS_NEW/'\n",
    "\n",
    "if not os.path.exists(BASE_MODEL_PATH):\n",
    "    os.mkdir(BASE_MODEL_PATH)\n",
    "if not os.path.exists(BASE_METRIC_PATH):\n",
    "    os.mkdir(BASE_METRIC_PATH)\n",
    "\n",
    "save_path = BASE_MODEL_PATH + model_structure_name + '/'\n",
    "save_metrics_path = BASE_METRIC_PATH\n",
    "\n",
    "if not os.path.exists(save_path):\n",
    "    os.mkdir(save_path)\n",
    "\n",
    "# Load datasets as PrepareDataset objects\n",
    "train = dt.PrepareDataset(dataframes=TRAINING_DATASETS, \n",
    "                          channel_mode=TRAINING_COLOR_MODE, \n",
    "                          scale=TRAINING_SCALE,\n",
    "                          batch_size=TRAINING_BATCH_SIZE, \n",
    "                          patch_size=TRAINING_PATCH_SIZE,\n",
    "                          data_augmentation=TRAINING_DATA_AUG, \n",
    "                          shuffle=TRAINING_SHUFFLE, \n",
    "                          repeat=TRAINING_REPEAT,\n",
    "                          normalize=TRAINING_NORMALIZE)\n",
    "\n",
    "val = dt.PrepareDataset(dataframes=VALIDATION_DATASETS, \n",
    "                        channel_mode=VALIDATION_COLOR_MODE, \n",
    "                        scale=VALIDATION_SCALE, \n",
    "                        batch_size=VALIDATION_BATCH_SIZE,\n",
    "                        patch_size=VALIDATION_PATCH_SIZE, \n",
    "                        data_augmentation=VALIDATION_DATA_AUG,\n",
    "                        shuffle=VALIDATION_SHUFFLE,\n",
    "                        repeat=VALIDATION_REPEAT,\n",
    "                        normalize=VALIDATION_NORMALIZE)\n",
    "\n",
    "print('----------------------------------')\n",
    "print('Num images for training:', sum(x.shape[0] for x in TRAINING_DATASETS))\n",
    "print('Num images for validating:', sum(x.shape[0] for x in VALIDATION_DATASETS))\n",
    "print('----------------------------------')\n",
    "\n",
    "def aux_fn(datasets, condition_fn):\n",
    "    return condition_fn(condition_fn(cv2.imread(row.path).shape[1:-1]) for i in range(len(datasets)) for row in datasets[i].itertuples())\n",
    "\n",
    "print('Smallest image in train dataset:', aux_fn(TRAINING_DATASETS, min))\n",
    "smallest_validation_size = aux_fn(VALIDATION_DATASETS, min)\n",
    "print('Smallest image in validation dataset:', smallest_validation_size)\n",
    "print('----------------------------------')\n",
    "print('Biggest image in train dataset:', aux_fn(TRAINING_DATASETS, max))\n",
    "biggest_validation_image = aux_fn(VALIDATION_DATASETS, max)\n",
    "print('Biggest image in validation dataset:', biggest_validation_image)\n",
    "print('----------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "registered-reward",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Num Conv layers: 814\n",
      "   Num ReLU layers: 200\n",
      "   Num Add layers: 211\n",
      "Total layers: 1225\n",
      "Num layers: 1631\n",
      "Num parameters: 15629283\n",
      "Num trainable variables: 1628\n"
     ]
    }
   ],
   "source": [
    "# Define model\n",
    "NUM_RESIDUAL_GROUPS = 10\n",
    "NUM_RESIDUAL_BLOCKS = 20\n",
    "NUM_FEATURES = 64\n",
    "KERNEL_SIZE = 3\n",
    "REDUCTION = 16\n",
    "NUM_IMAGE_CHANNELS = TRAINING_COLOR_MODE\n",
    "SCALE = TRAINING_SCALE\n",
    "NORMALIZATION = False\n",
    "TRAINING_LOOP = training_loops.Train_Loss_LR\n",
    "\n",
    "model = RCAN.get_RCAN(NUM_RESIDUAL_GROUPS,\n",
    "                        NUM_RESIDUAL_BLOCKS,\n",
    "                        NUM_FEATURES,\n",
    "                        KERNEL_SIZE,\n",
    "                        REDUCTION,\n",
    "                        NUM_IMAGE_CHANNELS,\n",
    "                        SCALE,\n",
    "                        NORMALIZATION,\n",
    "                        TRAINING_LOOP)\n",
    "\n",
    "# model.summary()\n",
    "types = {\n",
    "    'Conv': tf.keras.layers.Conv2D,\n",
    "    'ReLU': tf.keras.layers.ReLU,\n",
    "    'Add': tf.keras.layers.Add \n",
    "}\n",
    "\n",
    "total_num = 0\n",
    "\n",
    "for k in types:\n",
    "    num = len(list(filter(lambda x: type(x) == types[k], model.layers)))\n",
    "    print('   Num', k, 'layers:', num)\n",
    "    total_num += num\n",
    "print('Total layers:', total_num)\n",
    "\n",
    "print('Num layers:', len(model.layers))\n",
    "print('Num parameters:', model.count_params())\n",
    "\n",
    "print('Num trainable variables:', sum(len(l.trainable_variables) for l in model.layers))\n",
    "\n",
    "# Save model structure to JSON file\n",
    "if os.path.exists(save_path + model_name + '.json'):\n",
    "    os.remove(save_path + model_name + '.json')\n",
    "with open(save_path + model_name + '.json', 'w') as json_file:\n",
    "    json_file.write(model.to_json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "buried-philosophy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starts training\n",
      "Epoch 1/100\n",
      "10000/10000 [==============================] - 6354s 635ms/step - loss: 0.8432 - psnr: 22.8398 - ssim: 0.4255 - mse: 453.1130 - mae: 14.1792 - sobel_loss: 7977.6743 - val_loss: 0.5498 - val_psnr: 23.5288 - val_ssim: 0.4587 - val_mse: 315.7200 - val_mae: 11.9800 - val_sobel_loss: 6451.7632\n",
      "Epoch 2/100\n",
      "10000/10000 [==============================] - 6324s 632ms/step - loss: 0.2789 - psnr: 22.1444 - ssim: 0.3983 - mse: 467.7705 - mae: 15.0553 - sobel_loss: 9496.1348 - val_loss: 0.3131 - val_psnr: 20.7284 - val_ssim: 0.3426 - val_mse: 575.8200 - val_mae: 17.0600 - val_sobel_loss: 12050.0752\n",
      "Epoch 3/100\n",
      "10000/10000 [==============================] - 6316s 632ms/step - loss: 0.2228 - psnr: 19.5023 - ssim: 0.2898 - mse: 827.4491 - mae: 20.9522 - sobel_loss: 16966.1973 - val_loss: 0.1781 - val_psnr: 18.0772 - val_ssim: 0.2521 - val_mse: 1050.8300 - val_mae: 23.8100 - val_sobel_loss: 21287.2109\n",
      "Epoch 4/100\n",
      "10000/10000 [==============================] - 6323s 632ms/step - loss: 0.1929 - psnr: 17.4507 - ssim: 0.2252 - mse: 1310.6140 - mae: 26.8816 - sobel_loss: 26227.0312 - val_loss: 0.2185 - val_psnr: 16.3303 - val_ssim: 0.2053 - val_mse: 1571.9399 - val_mae: 29.4200 - val_sobel_loss: 30722.7598\n",
      "Epoch 5/100\n",
      "10000/10000 [==============================] - 6317s 632ms/step - loss: 0.1752 - psnr: 16.1798 - ssim: 0.1916 - mse: 1753.9556 - mae: 31.3089 - sobel_loss: 34301.8320 - val_loss: 0.2268 - val_psnr: 15.3106 - val_ssim: 0.1806 - val_mse: 1990.4200 - val_mae: 33.2900 - val_sobel_loss: 38050.1914\n",
      "Epoch 6/100\n",
      "10000/10000 [==============================] - 6316s 632ms/step - loss: 0.1668 - psnr: 15.4161 - ssim: 0.1733 - mse: 2093.4375 - mae: 34.3384 - sobel_loss: 40365.8438 - val_loss: 0.1119 - val_psnr: 14.6600 - val_ssim: 0.1662 - val_mse: 2316.2500 - val_mae: 35.9400 - val_sobel_loss: 43571.3633\n",
      "Epoch 7/100\n",
      "10000/10000 [==============================] - 6315s 632ms/step - loss: 0.1521 - psnr: 14.8335 - ssim: 0.1604 - mse: 2402.5703 - mae: 36.8014 - sobel_loss: 45731.1680 - val_loss: 0.1113 - val_psnr: 14.0869 - val_ssim: 0.1542 - val_mse: 2646.6699 - val_mae: 38.4200 - val_sobel_loss: 49065.6211\n",
      "Epoch 8/100\n",
      "10000/10000 [==============================] - 6337s 634ms/step - loss: 0.1404 - psnr: 14.4057 - ssim: 0.1518 - mse: 2657.7251 - mae: 38.6949 - sobel_loss: 50188.1680 - val_loss: 0.0804 - val_psnr: 13.6943 - val_ssim: 0.1462 - val_mse: 2900.1799 - val_mae: 40.2200 - val_sobel_loss: 53291.1836\n",
      "Epoch 9/100\n",
      " 1892/10000 [====>.........................] - ETA: 1:24:21 - loss: 0.1401 - psnr: 14.2071 - ssim: 0.1471 - mse: 2791.0518 - mae: 39.6295 - sobel_loss: 52437.0625"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-c988c6da3cb8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Starts training'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCBACKS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Ends training'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miguel_herrera/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miguel_herrera/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miguel_herrera/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miguel_herrera/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    805\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miguel_herrera/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miguel_herrera/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1841\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1842\u001b[0m     \"\"\"\n\u001b[0;32m-> 1843\u001b[0;31m     return self._call_flat(\n\u001b[0m\u001b[1;32m   1844\u001b[0m         [t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[1;32m   1845\u001b[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001b[0;32m~/miguel_herrera/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1921\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1923\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1924\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/miguel_herrera/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    546\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miguel_herrera/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Metrics for model evaluation\n",
    "METRICS = [\n",
    "    ctes.METRIC_FUNCTIONS.PSNR,\n",
    "    ctes.METRIC_FUNCTIONS.SSIM,\n",
    "    ctes.METRIC_FUNCTIONS.SSIM_MS, # ---- https://github.com/tensorflow/tensorflow/issues/33840\n",
    "                                   #      MODIFIED ARGUMENTS IN SSIM MULTISCALE (lib.custom_metric_functions.py)\n",
    "    ctes.METRIC_FUNCTIONS.MSE,\n",
    "    ctes.METRIC_FUNCTIONS.MAE,\n",
    "    ctes.METRIC_FUNCTIONS.SOBEL\n",
    "]\n",
    "\n",
    "# Metrics for saving model's checkpoints\n",
    "METRICS_CHECKPOINTS = [\n",
    "    (ctes.METRICS_ALL.PSNR, ctes.METRICS_ALL.VAL_PSNR, 'max'),\n",
    "    (ctes.METRICS_ALL.SSIM, ctes.METRICS_ALL.VAL_SSIM, 'max'),\n",
    "    # (ctes.METRICS_ALL.SOBEL, ctes.METRICS_ALL.VAL_SOBEL, 'min')\n",
    "]\n",
    "\n",
    "# Define training hyperparameters\n",
    "INITIAL_LEARNING_RATE = 0.0001 # 1e-4\n",
    "\n",
    "NUM_EPOCHS = 100 # Nº máximo de iteraciones de entrenamiento: 1.000.000 -- Como antes definimos una época como 10.000 iteraciones (repetir 200 veces el dataset de 800 imágenes dividio en batches de 16)\n",
    "                 # 1.000.000 de iteraciones serán 100 épocas\n",
    "\n",
    "OPTIMIZER = tf.keras.optimizers.Adam(learning_rate=INITIAL_LEARNING_RATE)\n",
    "LOSS_FUNCTION = train.get_loss_function(ctes.LOSS_FUNCTIONS.MAE)\n",
    "\n",
    "# Define callbacks\n",
    "\n",
    "# Se indica en el paper original, y se aplica en la implementación en TF 1.13, que cada 200.000 iteraciones, el learning rate se debe reducir a la mitad\n",
    "# Estableciendo que una época se compone de 200 repeticiones del dataset de entrenamiento para conseguir 10.000 iteraciones, cada 20 'épocas' se deberá reducir a la mitad el learning rate\n",
    "\n",
    "learning_rate_scheduler_callback = tf.keras.callbacks.LearningRateScheduler(callbacks.create_scheduler_function(20, 0.5)) # Cada 20 épocas, multiplicar lr por 0.5\n",
    "\n",
    "# Model checkpoints callbacks\n",
    "checkpoint_callbacks = [tf.keras.callbacks.ModelCheckpoint(save_path + model_name + '_best_' + mtr[0] + '.h5',\n",
    "                                                           monitor=mtr[1],\n",
    "                                                           save_best_only=True,\n",
    "                                                           mode=mtr[2],\n",
    "                                                           save_weights_only=True) for mtr in METRICS_CHECKPOINTS]\n",
    "\n",
    "# Save training metrics evolution callback\n",
    "metrics_evolution_callback = callbacks.Save_Training_Evolution(save_metrics_path + model_name_for_metrics + '_evolution.csv')\n",
    "\n",
    "\n",
    "CBACKS = [learning_rate_scheduler_callback, checkpoint_callbacks, metrics_evolution_callback] \n",
    "\n",
    "\n",
    "# TRAIN\n",
    "model.compile(optimizer=OPTIMIZER,\n",
    "              loss=LOSS_FUNCTION,\n",
    "              # SSIM_MS needs greater images than training patches, so ignore this metric on training\n",
    "              metrics=[train.get_metric_function(x) for x in METRICS if x != ctes.METRIC_FUNCTIONS.SSIM_MS])\n",
    "\n",
    "print('Starts training')\n",
    "model.fit(train.dataset, epochs=NUM_EPOCHS, verbose=1, validation_data=val.dataset, callbacks=CBACKS)\n",
    "print('Ends training')\n",
    "\n",
    "'''\n",
    "Starts training --- CUANDO SE CALCULABA SOBRE LAS LR\n",
    "Epoch 1/100\n",
    " 3566/10000 [=========>....................] - ETA: 1:06:45 - loss: 2.2201 - psnr: 41.8648 - ssim: 0.9845 - mse: 45.8110 - mae: 1.5339 - sobel_loss: 253.4401\n",
    "'''\n",
    "\n",
    "with open('ended_scripts.txt', 'a') as f:\n",
    "    f.write(FILENAME + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "affected-string",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('TRAINED_MODELS_NEW/RCAN-SCALE3/model_loss-computed-on-lr_last_epoch.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "language": "python",
   "name": "python38564bit6c4dec2e42734bc298ce0c3bfcfccb75"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
