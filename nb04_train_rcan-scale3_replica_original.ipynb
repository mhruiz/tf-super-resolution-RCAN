{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "north-jacob",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-14 21:16:34.913986: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2021-10-14 21:16:35.861367: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-10-14 21:16:35.861947: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2021-10-14 21:16:35.895787: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-14 21:16:35.896009: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: NVIDIA GeForce GTX 1650 computeCapability: 7.5\n",
      "coreClock: 1.56GHz coreCount: 16 deviceMemorySize: 3.82GiB deviceMemoryBandwidth: 119.24GiB/s\n",
      "2021-10-14 21:16:35.896023: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2021-10-14 21:16:35.897390: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2021-10-14 21:16:35.897422: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2021-10-14 21:16:35.898710: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2021-10-14 21:16:35.898901: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2021-10-14 21:16:35.900212: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2021-10-14 21:16:35.900934: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2021-10-14 21:16:35.904695: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2021-10-14 21:16:35.904787: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-14 21:16:35.905034: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-14 21:16:35.905212: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2021-10-14 21:16:35.905657: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-10-14 21:16:35.906824: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-14 21:16:35.907025: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: NVIDIA GeForce GTX 1650 computeCapability: 7.5\n",
      "coreClock: 1.56GHz coreCount: 16 deviceMemorySize: 3.82GiB deviceMemoryBandwidth: 119.24GiB/s\n",
      "2021-10-14 21:16:35.907039: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2021-10-14 21:16:35.907055: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2021-10-14 21:16:35.907065: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2021-10-14 21:16:35.907074: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2021-10-14 21:16:35.907084: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2021-10-14 21:16:35.907093: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2021-10-14 21:16:35.907105: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2021-10-14 21:16:35.907115: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2021-10-14 21:16:35.907153: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-14 21:16:35.907366: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-14 21:16:35.907540: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2021-10-14 21:16:35.907560: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2021-10-14 21:16:36.313921: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2021-10-14 21:16:36.313941: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \n",
      "2021-10-14 21:16:36.313945: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \n",
      "2021-10-14 21:16:36.314107: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-14 21:16:36.314395: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-14 21:16:36.314641: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-14 21:16:36.314850: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2160 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce GTX 1650, pci bus id: 0000:01:00.0, compute capability: 7.5)\n",
      "2021-10-14 21:16:36.315023: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n",
      "WARNING:tensorflow:AutoGraph could not transform <function get_patch_from_pair_of_images at 0x7fdd59b6c8c0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function get_patch_from_pair_of_images at 0x7fdd59b6c8c0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "----------------------------------\n",
      "Num images for training: 800\n",
      "Num images for validating: 100\n",
      "----------------------------------\n",
      "Smallest image in train dataset: 1116\n",
      "Smallest image in validation dataset: 1356\n",
      "----------------------------------\n",
      "Biggest image in train dataset: 2040\n",
      "Biggest image in validation dataset: 2040\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "# GPU memory limit\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)\n",
    "\n",
    "# Import own modules\n",
    "import lib.custom_callbacks as callbacks\n",
    "import lib.PrepareDataset as dt\n",
    "import lib.constants as ctes\n",
    "import lib.RCAN as RCAN\n",
    "\n",
    "######################################################################################\n",
    "# DEFINE ARGUMENTS\n",
    "\n",
    "# Load datasets from CSV files\n",
    "TRAINING_DATASETS = [pd.read_csv('data/0_csvs/DIV2K_train_HR.csv')]\n",
    "VALIDATION_DATASETS = [pd.read_csv('data/0_csvs/DIV2K_valid_HR.csv')]\n",
    "\n",
    "# Define dataset parameters\n",
    "TRAINING_COLOR_MODE = ctes.COLOR_MODE.RGB\n",
    "TRAINING_SCALE = 3\n",
    "TRAINING_BATCH_SIZE = 16\n",
    "TRAINING_PATCH_SIZE = 48\n",
    "TRAINING_DATA_AUG = True\n",
    "TRAINING_SHUFFLE = True\n",
    "TRAINING_REPEAT = 200 # ----------------- Según la implementación en TF 1.13, se valida cada 10.000 training steps. Al ser 800 imágenes de entrenamiento y un batch de 16, hay 50 steps por época\n",
    "                      #                   Hay que realizar 10.000 iteraciones antes de validar. Estas son equivalentes a 200 épocas de entrenamiento sin validación, osea, repetir 200 veces el dataset\n",
    "TRAINING_NORMALIZE = False\n",
    "\n",
    "VALIDATION_COLOR_MODE = TRAINING_COLOR_MODE\n",
    "VALIDATION_SCALE = TRAINING_SCALE\n",
    "VALIDATION_BATCH_SIZE = 1\n",
    "VALIDATION_PATCH_SIZE = None\n",
    "VALIDATION_DATA_AUG = False\n",
    "VALIDATION_SHUFFLE = False\n",
    "VALIDATION_REPEAT = None\n",
    "VALIDATION_NORMALIZE = False\n",
    "\n",
    "FILENAME = 'nb04_train_rcan-scale3_replica_original'\n",
    "\n",
    "# Get model info from file name\n",
    "info_from_script_name = FILENAME.split('_') \n",
    "\n",
    "model_structure_name = info_from_script_name[2].upper() \n",
    "\n",
    "model_name = info_from_script_name[3:]            \n",
    "model_name = ('_'.join(['model'] + model_name)).split('.')[0]\n",
    "model_name_for_metrics = model_structure_name + '_' + model_name         \n",
    "\n",
    "BASE_MODEL_PATH = 'TRAINED_MODELS_NEW/'\n",
    "BASE_METRIC_PATH = 'TRAINING_METRIC_EVOLUTIONS_NEW/'\n",
    "\n",
    "if not os.path.exists(BASE_MODEL_PATH):\n",
    "    os.mkdir(BASE_MODEL_PATH)\n",
    "if not os.path.exists(BASE_METRIC_PATH):\n",
    "    os.mkdir(BASE_METRIC_PATH)\n",
    "\n",
    "save_path = BASE_MODEL_PATH + model_structure_name + '/'\n",
    "save_metrics_path = BASE_METRIC_PATH\n",
    "\n",
    "if not os.path.exists(save_path):\n",
    "    os.mkdir(save_path)\n",
    "\n",
    "# Load datasets as PrepareDataset objects\n",
    "train = dt.PrepareDataset(dataframes=TRAINING_DATASETS, \n",
    "                          channel_mode=TRAINING_COLOR_MODE, \n",
    "                          scale=TRAINING_SCALE,\n",
    "                          batch_size=TRAINING_BATCH_SIZE, \n",
    "                          patch_size=TRAINING_PATCH_SIZE,\n",
    "                          data_augmentation=TRAINING_DATA_AUG, \n",
    "                          shuffle=TRAINING_SHUFFLE, \n",
    "                          repeat=TRAINING_REPEAT,\n",
    "                          normalize=TRAINING_NORMALIZE)\n",
    "\n",
    "val = dt.PrepareDataset(dataframes=VALIDATION_DATASETS, \n",
    "                        channel_mode=VALIDATION_COLOR_MODE, \n",
    "                        scale=VALIDATION_SCALE, \n",
    "                        batch_size=VALIDATION_BATCH_SIZE,\n",
    "                        patch_size=VALIDATION_PATCH_SIZE, \n",
    "                        data_augmentation=VALIDATION_DATA_AUG,\n",
    "                        shuffle=VALIDATION_SHUFFLE,\n",
    "                        repeat=VALIDATION_REPEAT,\n",
    "                        normalize=VALIDATION_NORMALIZE)\n",
    "\n",
    "print('----------------------------------')\n",
    "print('Num images for training:', sum(x.shape[0] for x in TRAINING_DATASETS))\n",
    "print('Num images for validating:', sum(x.shape[0] for x in VALIDATION_DATASETS))\n",
    "print('----------------------------------')\n",
    "\n",
    "def aux_fn(datasets, condition_fn):\n",
    "    return condition_fn(condition_fn(cv2.imread(row.path).shape[1:-1]) for i in range(len(datasets)) for row in datasets[i].itertuples())\n",
    "\n",
    "print('Smallest image in train dataset:', aux_fn(TRAINING_DATASETS, min))\n",
    "smallest_validation_size = aux_fn(VALIDATION_DATASETS, min)\n",
    "print('Smallest image in validation dataset:', smallest_validation_size)\n",
    "print('----------------------------------')\n",
    "print('Biggest image in train dataset:', aux_fn(TRAINING_DATASETS, max))\n",
    "biggest_validation_image = aux_fn(VALIDATION_DATASETS, max)\n",
    "print('Biggest image in validation dataset:', biggest_validation_image)\n",
    "print('----------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "younger-match",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Num Conv layers: 814\n",
      "   Num ReLU layers: 200\n",
      "   Num Add layers: 211\n",
      "Total layers: 1225\n",
      "Num layers: 1631\n",
      "Num parameters: 15629283\n",
      "Num trainable variables: 1628\n"
     ]
    }
   ],
   "source": [
    "# Define model\n",
    "NUM_RESIDUAL_GROUPS = 10\n",
    "NUM_RESIDUAL_BLOCKS = 20\n",
    "NUM_FEATURES = 64\n",
    "KERNEL_SIZE = 3\n",
    "REDUCTION = 16\n",
    "NUM_IMAGE_CHANNELS = TRAINING_COLOR_MODE # 3\n",
    "SCALE = TRAINING_SCALE\n",
    "NORMALIZATION = False\n",
    "\n",
    "model = RCAN.get_RCAN(NUM_RESIDUAL_GROUPS,\n",
    "                        NUM_RESIDUAL_BLOCKS,\n",
    "                        NUM_FEATURES,\n",
    "                        KERNEL_SIZE,\n",
    "                        REDUCTION,\n",
    "                        NUM_IMAGE_CHANNELS,\n",
    "                        SCALE,\n",
    "                        NORMALIZATION)\n",
    "\n",
    "# model.summary()\n",
    "types = {\n",
    "    'Conv': tf.keras.layers.Conv2D,\n",
    "    'ReLU': tf.keras.layers.ReLU,\n",
    "    'Add': tf.keras.layers.Add \n",
    "}\n",
    "\n",
    "total_num = 0\n",
    "\n",
    "for k in types:\n",
    "    num = len(list(filter(lambda x: type(x) == types[k], model.layers)))\n",
    "    print('   Num', k, 'layers:', num)\n",
    "    total_num += num\n",
    "print('Total layers:', total_num)\n",
    "\n",
    "print('Num layers:', len(model.layers))\n",
    "print('Num parameters:', model.count_params())\n",
    "\n",
    "print('Num trainable variables:', sum(len(l.trainable_variables) for l in model.layers))\n",
    "\n",
    "# Save model structure to JSON file\n",
    "if os.path.exists(save_path + model_name + '.json'):\n",
    "    os.remove(save_path + model_name + '.json')\n",
    "with open(save_path + model_name + '.json', 'w') as json_file:\n",
    "    json_file.write(model.to_json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vanilla-weather",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starts training\n",
      "Epoch 1/100\n",
      "10000/10000 [==============================] - 6222s 622ms/step - loss: 6.0521 - psnr: 32.0584 - ssim: 0.8407 - mse: 138.2223 - mae: 5.5103 - sobel_loss: 2909.7810 - val_loss: 5.1866 - val_psnr: 30.2280 - val_ssim: 0.8599 - val_mse: 100.4500 - val_mae: 4.6600 - val_sobel_loss: 2190.8899\n",
      "Epoch 2/100\n",
      "10000/10000 [==============================] - 6181s 618ms/step - loss: 5.3709 - psnr: 33.2622 - ssim: 0.8618 - mse: 113.0640 - mae: 4.8426 - sobel_loss: 2437.6365 - val_loss: 4.9715 - val_psnr: 30.5703 - val_ssim: 0.8665 - val_mse: 93.5300 - val_mae: 4.4900 - val_sobel_loss: 2027.6069\n",
      "Epoch 3/100\n",
      "10000/10000 [==============================] - 6186s 619ms/step - loss: 5.1908 - psnr: 33.5848 - ssim: 0.8675 - mse: 106.5756 - mae: 4.6667 - sobel_loss: 2280.1675 - val_loss: 4.8683 - val_psnr: 30.7403 - val_ssim: 0.8699 - val_mse: 90.0800 - val_mae: 4.3900 - val_sobel_loss: 1944.4380\n",
      "Epoch 4/100\n",
      "10000/10000 [==============================] - 6190s 619ms/step - loss: 5.1111 - psnr: inf - ssim: 0.8700 - mse: 103.4267 - mae: 4.5845 - sobel_loss: 2205.0935 - val_loss: 4.9157 - val_psnr: 30.7351 - val_ssim: 0.8696 - val_mse: 89.0700 - val_mae: 4.4400 - val_sobel_loss: 1910.5786\n",
      "Epoch 5/100\n",
      "10000/10000 [==============================] - 6192s 619ms/step - loss: 5.0514 - psnr: inf - ssim: 0.8718 - mse: 100.9246 - mae: 4.5249 - sobel_loss: 2144.9866 - val_loss: 4.7967 - val_psnr: 30.8558 - val_ssim: 0.8720 - val_mse: 87.4300 - val_mae: 4.2900 - val_sobel_loss: 1882.1049\n",
      "Epoch 6/100\n",
      "10000/10000 [==============================] - 6183s 618ms/step - loss: 5.0443 - psnr: 33.8576 - ssim: 0.8724 - mse: 100.5962 - mae: 4.5187 - sobel_loss: 2130.8401 - val_loss: 4.7773 - val_psnr: 30.9065 - val_ssim: 0.8729 - val_mse: 86.5000 - val_mae: 4.2400 - val_sobel_loss: 1859.9604\n",
      "Epoch 7/100\n",
      "10000/10000 [==============================] - 6185s 618ms/step - loss: 5.0014 - psnr: 33.9257 - ssim: 0.8735 - mse: 99.0065 - mae: 4.4741 - sobel_loss: 2092.5325 - val_loss: 4.7484 - val_psnr: 30.9480 - val_ssim: 0.8735 - val_mse: 85.6700 - val_mae: 4.2200 - val_sobel_loss: 1838.0808\n",
      "Epoch 8/100\n",
      "10000/10000 [==============================] - 6191s 619ms/step - loss: 4.9671 - psnr: inf - ssim: 0.8746 - mse: 97.4847 - mae: 4.4404 - sobel_loss: 2059.4404 - val_loss: 4.7620 - val_psnr: 30.9530 - val_ssim: 0.8738 - val_mse: 85.5900 - val_mae: 4.2200 - val_sobel_loss: 1836.2150\n",
      "Epoch 9/100\n",
      "10000/10000 [==============================] - 6192s 619ms/step - loss: 4.9633 - psnr: inf - ssim: 0.8750 - mse: 97.6183 - mae: 4.4408 - sobel_loss: 2058.3369 - val_loss: 4.7231 - val_psnr: 30.9975 - val_ssim: 0.8748 - val_mse: 84.8200 - val_mae: 4.1500 - val_sobel_loss: 1817.0249\n",
      "Epoch 10/100\n",
      "10000/10000 [==============================] - 6186s 619ms/step - loss: 4.9407 - psnr: inf - ssim: 0.8755 - mse: 96.7953 - mae: 4.4119 - sobel_loss: 2037.8560 - val_loss: 4.6998 - val_psnr: 31.0361 - val_ssim: 0.8751 - val_mse: 84.4700 - val_mae: 4.1300 - val_sobel_loss: 1807.1857\n",
      "Epoch 11/100\n",
      "10000/10000 [==============================] - 6190s 619ms/step - loss: 4.9418 - psnr: inf - ssim: 0.8755 - mse: 96.5331 - mae: 4.4127 - sobel_loss: 2031.7024 - val_loss: 4.7863 - val_psnr: 30.9440 - val_ssim: 0.8738 - val_mse: 85.4000 - val_mae: 4.2600 - val_sobel_loss: 1823.8608\n",
      "Epoch 12/100\n",
      "10000/10000 [==============================] - 6191s 619ms/step - loss: 4.9183 - psnr: inf - ssim: 0.8763 - mse: 95.7149 - mae: 4.3962 - sobel_loss: 2013.9467 - val_loss: 4.7234 - val_psnr: 31.0227 - val_ssim: 0.8752 - val_mse: 84.5700 - val_mae: 4.1600 - val_sobel_loss: 1807.8878\n",
      "Epoch 13/100\n",
      "10000/10000 [==============================] - 6193s 619ms/step - loss: 4.8926 - psnr: inf - ssim: 0.8769 - mse: 94.9928 - mae: 4.3710 - sobel_loss: 1995.3564 - val_loss: 4.7169 - val_psnr: 31.0118 - val_ssim: 0.8746 - val_mse: 84.8500 - val_mae: 4.1600 - val_sobel_loss: 1814.6073\n",
      "Epoch 14/100\n",
      "10000/10000 [==============================] - 6199s 620ms/step - loss: 4.8882 - psnr: inf - ssim: 0.8769 - mse: 94.7398 - mae: 4.3610 - sobel_loss: 1989.2716 - val_loss: 4.6906 - val_psnr: 31.0539 - val_ssim: 0.8754 - val_mse: 83.9900 - val_mae: 4.1200 - val_sobel_loss: 1799.3292\n",
      "Epoch 15/100\n",
      "10000/10000 [==============================] - 6212s 621ms/step - loss: 4.8765 - psnr: inf - ssim: 0.8771 - mse: 94.0745 - mae: 4.3567 - sobel_loss: 1974.3866 - val_loss: 4.7518 - val_psnr: 30.9755 - val_ssim: 0.8744 - val_mse: 84.6900 - val_mae: 4.2000 - val_sobel_loss: 1812.4430\n",
      "Epoch 16/100\n",
      "10000/10000 [==============================] - 6209s 621ms/step - loss: 4.8706 - psnr: inf - ssim: 0.8774 - mse: 94.1045 - mae: 4.3429 - sobel_loss: 1973.5752 - val_loss: 4.6795 - val_psnr: 31.0788 - val_ssim: 0.8762 - val_mse: 83.6200 - val_mae: 4.1000 - val_sobel_loss: 1789.4396\n",
      "Epoch 17/100\n",
      "10000/10000 [==============================] - 6209s 621ms/step - loss: 4.8604 - psnr: inf - ssim: 0.8778 - mse: 93.7455 - mae: 4.3410 - sobel_loss: 1965.3182 - val_loss: 4.7062 - val_psnr: 31.0064 - val_ssim: 0.8751 - val_mse: 85.1700 - val_mae: 4.1400 - val_sobel_loss: 1825.9351\n",
      "Epoch 18/100\n",
      "10000/10000 [==============================] - 6227s 623ms/step - loss: 4.8593 - psnr: inf - ssim: 0.8780 - mse: 93.8092 - mae: 4.3338 - sobel_loss: 1966.2704 - val_loss: 4.6703 - val_psnr: 31.0850 - val_ssim: 0.8760 - val_mse: 83.5500 - val_mae: 4.1000 - val_sobel_loss: 1785.8135\n",
      "Epoch 19/100\n",
      "10000/10000 [==============================] - 6221s 622ms/step - loss: 4.8497 - psnr: inf - ssim: 0.8779 - mse: 93.1093 - mae: 4.3256 - sobel_loss: 1950.5696 - val_loss: 4.6985 - val_psnr: 31.0573 - val_ssim: 0.8756 - val_mse: 84.0900 - val_mae: 4.1300 - val_sobel_loss: 1797.8185\n",
      "Epoch 20/100\n",
      "10000/10000 [==============================] - 6241s 624ms/step - loss: 4.8434 - psnr: inf - ssim: 0.8783 - mse: 92.9537 - mae: 4.3164 - sobel_loss: 1946.3490 - val_loss: 4.6685 - val_psnr: 31.0864 - val_ssim: 0.8763 - val_mse: 83.6500 - val_mae: 4.0900 - val_sobel_loss: 1787.8398\n",
      "Epoch 0020 --- Learning rate reduced to: 4.999999873689376e-05\n",
      "Epoch 21/100\n",
      "10000/10000 [==============================] - 6272s 627ms/step - loss: 4.7993 - psnr: inf - ssim: 0.8796 - mse: 91.6728 - mae: 4.2797 - sobel_loss: 1915.8152 - val_loss: 4.6298 - val_psnr: 31.1686 - val_ssim: 0.8774 - val_mse: 82.2600 - val_mae: 4.0700 - val_sobel_loss: 1755.6261\n",
      "Epoch 22/100\n",
      "10000/10000 [==============================] - 6287s 629ms/step - loss: 4.7829 - psnr: inf - ssim: 0.8800 - mse: 91.0044 - mae: 4.2617 - sobel_loss: 1900.0636 - val_loss: 4.6278 - val_psnr: 31.1762 - val_ssim: 0.8773 - val_mse: 82.0300 - val_mae: 4.0700 - val_sobel_loss: 1751.1941\n",
      "Epoch 23/100\n",
      "10000/10000 [==============================] - 6299s 630ms/step - loss: 4.7685 - psnr: inf - ssim: 0.8805 - mse: 90.7793 - mae: 4.2456 - sobel_loss: 1894.4600 - val_loss: 4.6487 - val_psnr: 31.1631 - val_ssim: 0.8774 - val_mse: 82.3700 - val_mae: 4.0700 - val_sobel_loss: 1753.7762\n",
      "Epoch 24/100\n",
      "10000/10000 [==============================] - 6300s 630ms/step - loss: 4.7782 - psnr: inf - ssim: 0.8800 - mse: 90.8748 - mae: 4.2562 - sobel_loss: 1895.3074 - val_loss: 4.6268 - val_psnr: 31.1858 - val_ssim: 0.8776 - val_mse: 81.9100 - val_mae: 4.0700 - val_sobel_loss: 1747.3450\n",
      "Epoch 25/100\n",
      "10000/10000 [==============================] - 6319s 632ms/step - loss: 4.7914 - psnr: inf - ssim: 0.8800 - mse: 91.3081 - mae: 4.2722 - sobel_loss: 1904.9100 - val_loss: 4.6321 - val_psnr: 31.1646 - val_ssim: 0.8772 - val_mse: 82.3200 - val_mae: 4.0700 - val_sobel_loss: 1756.8425\n",
      "Epoch 26/100\n",
      " 4031/10000 [===========>..................] - ETA: 1:01:58 - loss: 4.7656 - psnr: inf - ssim: 0.8806 - mse: 90.3781 - mae: 4.2399 - sobel_loss: 1884.1327"
     ]
    }
   ],
   "source": [
    "# Metrics for model evaluation\n",
    "METRICS = [\n",
    "    ctes.METRIC_FUNCTIONS.PSNR,\n",
    "    ctes.METRIC_FUNCTIONS.SSIM,\n",
    "    ctes.METRIC_FUNCTIONS.SSIM_MS, # ---- https://github.com/tensorflow/tensorflow/issues/33840\n",
    "                                   #      MODIFIED ARGUMENTS IN SSIM MULTISCALE (lib.custom_metric_functions.py)\n",
    "    ctes.METRIC_FUNCTIONS.MSE,\n",
    "    ctes.METRIC_FUNCTIONS.MAE,\n",
    "    ctes.METRIC_FUNCTIONS.SOBEL\n",
    "]\n",
    "\n",
    "# Metrics for saving model's checkpoints\n",
    "METRICS_CHECKPOINTS = [\n",
    "    (ctes.METRICS_ALL.PSNR, ctes.METRICS_ALL.VAL_PSNR, 'max'),\n",
    "    (ctes.METRICS_ALL.SSIM, ctes.METRICS_ALL.VAL_SSIM, 'max'),\n",
    "    # (ctes.METRICS_ALL.SOBEL, ctes.METRICS_ALL.VAL_SOBEL, 'min')\n",
    "]\n",
    "\n",
    "# Define training hyperparameters\n",
    "INITIAL_LEARNING_RATE = 0.0001\n",
    "\n",
    "NUM_EPOCHS = 100 # Nº máximo de iteraciones de entrenamiento: 1.000.000 -- Como antes definimos una época como 10.000 iteraciones (repetir 200 veces el dataset de 800 imágenes dividio en batches de 16)\n",
    "                 # 1.000.000 de iteraciones serán 100 épocas\n",
    "\n",
    "OPTIMIZER = tf.keras.optimizers.Adam(learning_rate=INITIAL_LEARNING_RATE)\n",
    "LOSS_FUNCTION = train.get_loss_function(ctes.LOSS_FUNCTIONS.MAE)\n",
    "\n",
    "# Define callbacks\n",
    "\n",
    "# Se indica en el paper original, y se aplica en la implementación en TF 1.13, que cada 200.000 iteraciones, el learning rate se debe reducir a la mitad\n",
    "# Estableciendo que una época se compone de 200 repeticiones del dataset de entrenamiento para conseguir 10.000 iteraciones, cada 20 'épocas' se deberá reducir a la mitad el learning rate\n",
    "\n",
    "learning_rate_scheduler_callback = tf.keras.callbacks.LearningRateScheduler(callbacks.create_scheduler_function(20, 0.5)) # Cada 20 épocas, multiplicar lr por 0.5\n",
    "\n",
    "# Model checkpoints callbacks\n",
    "checkpoint_callbacks = [tf.keras.callbacks.ModelCheckpoint(save_path + model_name + '_best_' + mtr[0] + '.h5',\n",
    "                                                           monitor=mtr[1],\n",
    "                                                           save_best_only=True,\n",
    "                                                           mode=mtr[2],\n",
    "                                                           save_weights_only=True) for mtr in METRICS_CHECKPOINTS]\n",
    "\n",
    "# Save training metrics evolution callback\n",
    "metrics_evolution_callback = callbacks.Save_Training_Evolution(save_metrics_path + model_name_for_metrics + '_evolution.csv')\n",
    "\n",
    "\n",
    "CBACKS = [learning_rate_scheduler_callback, checkpoint_callbacks, metrics_evolution_callback] \n",
    "\n",
    "\n",
    "# TRAIN\n",
    "model.compile(optimizer=OPTIMIZER,\n",
    "              loss=LOSS_FUNCTION,\n",
    "              # SSIM_MS needs greater images than training patches, so ignore this metric on training\n",
    "              metrics=[train.get_metric_function(x) for x in METRICS if x != ctes.METRIC_FUNCTIONS.SSIM_MS])\n",
    "\n",
    "print('Starts training')\n",
    "model.fit(train.dataset, epochs=NUM_EPOCHS, verbose=1, validation_data=val.dataset, callbacks=CBACKS)\n",
    "print('Ends training')\n",
    "\n",
    "\n",
    "with open('ended_scripts.txt', 'a') as f:\n",
    "    f.write(FILENAME + '\\n')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7ecfc84cf627d4668eb5049ae69c5fc175d9214e35be9106b7b58ba4194e3257"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('TF24': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
